{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYiZq0X2oB5t"
   },
   "source": [
    "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
    "\n",
    "# **HW1a The Perceptron** (20 pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGVmKzgG2Ium",
    "outputId": "4cc2ca21-861a-4fba-a38c-83e3ec04bec8"
   },
   "outputs": [],
   "source": [
    "# Get the datasets\n",
    "def print_file_head(file_path, lines=5):\n",
    "    \"\"\"Reads and prints the first few lines of a text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            head = [next(file) for _ in range(lines)]\n",
    "            print(f\"Head of {file_path}:\\n{''.join(head)}\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found.\")\n",
    "    except StopIteration:\n",
    "        print(f\"Error: {file_path} is empty.\")\n",
    "\n",
    "# File paths\n",
    "test_small_path = r\"C:\\Users\\mukth\\Downloads\\test_small (1).txt\"\n",
    "train_path = r\"C:\\Users\\mukth\\Downloads\\train (1).txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A69DxPSc8vNs",
    "outputId": "5440e602-8ecd-44cf-d48d-2e8b00cdcc52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of C:\\Users\\mukth\\Downloads\\test_small (1).txt:\n",
      "X1\tX2\tX3\n",
      "1\t1\t1\t1\n",
      "0\t0\t1\t1\n",
      "0\t1\t1\t0\n",
      "0\t1\t1\t0\n",
      "\n",
      "\n",
      "Head of C:\\Users\\mukth\\Downloads\\train (1).txt:\n",
      "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
      "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a peek at the datasets\n",
    "print_file_head(test_small_path)\n",
    "print_file_head(train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFXHLhnhwiBR"
   },
   "source": [
    "### Build the Perceptron Model\n",
    "\n",
    "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cXAsP_lw3QwJ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "\n",
    "# Corpus reader, all columns but the last one are coordinates;\n",
    "#   the last column is the label\n",
    "def read_data(file_name):\n",
    "    f = open(file_name, 'r')\n",
    "\n",
    "    data = []\n",
    "    # Discard header line\n",
    "    f.readline()\n",
    "    for instance in f.readlines():\n",
    "        if not re.search('\\t', instance): continue\n",
    "        instance = list(map(int, instance.strip().split('\\t')))\n",
    "        # Add a dummy input so that w0 becomes the bias\n",
    "        instance = [-1] + instance\n",
    "        data += [instance]\n",
    "    return data\n",
    "\n",
    "\n",
    "def dot_product(array1, array2):\n",
    "    #TODO: Return dot product of array 1 and array \n",
    "    return sum(a * b for a, b in zip(array1, array2))\n",
    "    return NotImplemented \n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    #TODO: Return outpout of sigmoid function on x\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "    return NotImplemented\n",
    "\n",
    "# The output of the model, which for the perceptron is \n",
    "# the sigmoid function applied to the dot product of \n",
    "# the instance and the weights\n",
    "def output(weight, instance):\n",
    "    #TODO: return the output of the model \n",
    "    return sigmoid(dot_product(weights, instance))\n",
    "    return NotImplemented\n",
    "\n",
    "# Predict the label of an instance; this is the definition of the perceptron\n",
    "# you should output 1 if the output is >= 0.5 else output 0\n",
    "def predict(weights, instance):\n",
    "    #TODO: return the prediction of the model\n",
    "    return 1 if output(weights, instance) >= 0.5 else 0\n",
    "\n",
    "\n",
    "# Accuracy = percent of correct predictions\n",
    "def get_accuracy(weights, instances):\n",
    "    # You do not to write code like this, but get used to it\n",
    "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
    "                   for instance in instances])\n",
    "    return correct * 100 / len(instances)\n",
    "\n",
    "\n",
    "# Train a perceptron with instances and hyperparameters:\n",
    "#       lr (learning rate) \n",
    "#       epochs\n",
    "# The implementation comes from the definition of the perceptron\n",
    "#\n",
    "# Training consists on fitting the parameters which are the weights\n",
    "# that's the only thing training is responsible to fit\n",
    "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
    "#\n",
    "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
    "# We are updating weights in the opposite direction of the gradient of the error,\n",
    "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
    "def train_perceptron(instances, lr, epochs):\n",
    "    \"\"\"Train a perceptron using the given instances, learning rate (lr), and number of epochs.\"\"\"\n",
    "    weights = [0] * (len(instances[0]) - 1)  # Initialize weights (one less than the number of columns)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for instance in instances:\n",
    "            in_value = dot_product(weights, instance)  # Compute the dot product\n",
    "            out_value = sigmoid(in_value)  # Apply sigmoid to get output\n",
    "            error = instance[-1] - out_value  # Compute error (label - predicted value)\n",
    "\n",
    "            # Update weights using the gradient descent rule\n",
    "            for i in range(len(weights)):\n",
    "                weights[i] += lr * error * out_value * (1 - out_value) * instance[i]  # Weight update rule\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adBZuMlAwiBT"
   },
   "source": [
    "## Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "50YvUza-BYQF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(r\"C:\\Users\\mukth\\Downloads\\train (1).txt\")\n",
    "instances_te = read_data(r\"C:\\Users\\mukth\\Downloads\\test_small (1).txt\")\n",
    "lr = 0.005\n",
    "epochs = 5\n",
    "weights = train_perceptron(instances_tr, lr, epochs)\n",
    "accuracy = get_accuracy(weights, instances_te)\n",
    "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBXkvaiQMohX"
   },
   "source": [
    "## Questions\n",
    "\n",
    "Answer the following questions. Include your implementation and the output for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCQ6BEk1CBlr"
   },
   "source": [
    "\n",
    "\n",
    "### Question 1\n",
    " \n",
    "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
    "```\n",
    "in_value = dot_product(weights, instance)\n",
    "output = sigmoid(in_value)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "Why don't we have the following code snippet instead?\n",
    "```\n",
    "output = predict(weights, instance)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "#### TODO Add your answer here (text only)\n",
    "The dot product calculates the weighted sum of inputs.\n",
    "The sigmoid function is applied to generate a smooth, continuous output between 0 and 1.\n",
    "The error is determined from this output, allowing for proper gradient-based weight updates.\n",
    "The predict() function typically applies a threshold (e.g., returns 1 if output â‰¥ 0.5, else 0).\n",
    "This would make the output discrete (0 or 1), which is not suitable for training.\n",
    "Gradient-based learning requires a continuous output (from sigmoid) in order to compute meaningful updates.\n",
    "A discrete output would have zero gradients almost everywhere, and learning would not be possible.\n",
    "We use sigmoid(dot_product(.)) while training in order to maintain smooth learning, whereas predict() is used only for ultimate classification after training is complete.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU3c3m6YL2rK"
   },
   "source": [
    "### Question 2\n",
    "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
    "\n",
    "```\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
    "lr = [0.005, 0.01, 0.05]              # learning rate\n",
    "```\n",
    "\n",
    "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
    "of your code.The output should look like the following:\n",
    "```\n",
    "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "[and so on for all the combinations]\n",
    "```\n",
    "You will get different results with different hyperparameters.\n",
    "\n",
    "#### TODO Add your answer here (code and output in the format above) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "G-VKJOUu2BTp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tr: 20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 20, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 20, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 20, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 20, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 20, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 20, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 20, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 20, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 20, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 40, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 40, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 40, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 40, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 40, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 40, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 40, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 40, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 40, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 40, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 40, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 40, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "# tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def train_and_evaluate(train_data, test_data, tr_percent, num_epochs, lr):\n",
    "    \"\"\"Trains the perceptron on different training sizes and evaluates it.\"\"\"\n",
    "    train_size = int(len(train_data) * (tr_percent / 100))\n",
    "    train_subset = random.sample(train_data, train_size)  # Select a subset of training data\n",
    "    \n",
    "    weights = train_perceptron(train_subset, lr, num_epochs)  # Train the perceptron\n",
    "    accuracy = get_accuracy(weights, test_data)  # Compute accuracy on test set\n",
    "    \n",
    "    print(f\"# tr: {train_size}, epochs: {num_epochs:3}, learning rate: {lr:.3f}; Accuracy (test, {len(test_data)} instances): {accuracy:.1f}\")\n",
    "\n",
    "# Load datasets\n",
    "train_data = read_data(r\"C:\\Users\\mukth\\Downloads\\train (1).txt\")  # Assuming training data is in train.txt\n",
    "test_data = read_data(r\"C:\\Users\\mukth\\Downloads\\test (1).txt\")    # Assuming test data is in test.txt\n",
    "\n",
    "# Define hyperparameter values\n",
    "tr_percentages = [5, 10, 25, 50, 75, 100]\n",
    "num_epochs_list = [5, 10, 20, 50, 100]\n",
    "lr_values = [0.005, 0.01, 0.05]\n",
    "\n",
    "# Run training and evaluation for each combination of hyperparameters\n",
    "for tr_percent in tr_percentages:\n",
    "    for num_epochs in num_epochs_list:\n",
    "        for lr in lr_values:\n",
    "            train_and_evaluate(train_data, test_data, tr_percent, num_epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 42.9\n",
      "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 28.6\n",
      "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
      "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
      "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
      "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(r\"C:\\Users\\mukth\\Downloads\\train (1).txt\")\n",
    "instances_te = read_data(r\"C:\\Users\\mukth\\Downloads\\test_small (1).txt\")\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
    "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
    "\n",
    "for lr in lr_array:\n",
    "  for tr_size in tr_percent:\n",
    "    for epochs in num_epochs:\n",
    "      size =  round(len(instances_tr)*tr_size/100)\n",
    "      pre_instances = instances_tr[0:size]\n",
    "      weights = train_perceptron(pre_instances, lr, epochs)\n",
    "      accuracy = get_accuracy(weights, instances_te)\n",
    "    print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFB9MtwML24O"
   },
   "source": [
    "### Question 3\n",
    "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
    "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
    "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
    "   ```\n",
    "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "```\n",
    "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
    "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
    "\n",
    "#### TODO: Add your answer here (code and text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 42.9\n",
      "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 28.6\n",
      "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
      "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
      "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
      "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load datasets\n",
    "train_data = read_data(r\"C:\\Users\\mukth\\Downloads\\train (1).txt\")  # Assuming training data is in train.txt\n",
    "test_data = read_data(r\"C:\\Users\\mukth\\Downloads\\test (1).txt\")   \n",
    "\n",
    "train_data\n",
    "def train_and_evaluate(train_data, test_data, tr_percent, num_epochs, lr):\n",
    "    \"\"\"Trains the perceptron on different training sizes and evaluates it.\"\"\"\n",
    "    train_size = int(len(train_data) * (tr_percent / 100))\n",
    "    train_subset = random.sample(train_data, train_size)  # Select a subset of training data\n",
    "    \n",
    "    \n",
    "    weights = train_perceptron(train_subset, lr, num_epochs)  # Train the perceptron\n",
    "    accuracy = get_accuracy(weights, test_data)  # Compute accuracy on test set\n",
    "    \n",
    "    print(f\"# tr: {train_size}, epochs: {num_epochs:3}, learning rate: {lr:.3f}; Accuracy (test, {len(test_data)} instances): {accuracy:.1f}\")\n",
    "    return train_size, num_epochs, lr, accuracy\n",
    "\n",
    " # Assuming test data is in test.txt\n",
    "\n",
    "# Define hyperparameter values\n",
    "tr_percentages = [5, 10, 25, 50, 75, 100]\n",
    "num_epochs_list = [5, 10, 20, 50, 100]\n",
    "lr_values = [0.005, 0.01, 0.05]\n",
    "\n",
    "# Store results for plotting\n",
    "results = []\n",
    "# Run training and evaluation for each combination of hyperparameters\n",
    "for lr in lr_values:\n",
    "  for tr_size in tr_percentages:\n",
    "    for epochs in num_epochs_list:\n",
    "      size =  round(len(instances_tr)*tr_size/100)\n",
    "      pre_instances = instances_tr[0:size]\n",
    "      weights = train_perceptron(pre_instances, lr, epochs)\n",
    "      accuracy = get_accuracy(weights, instances_te)\n",
    "    print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")\n",
    "\n",
    "# Ensure plt is defined before plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A. Not necessarily. While using more training data often improves generalization, it depends on factors like data quality, model complexity, and hyperparameters. If the additional training data is noisy, redundant, or not representative, it may not improve test accuracy. Also, poorly chosen hyperparameters can lead to suboptimal results even with more data.\n",
    "\n",
    "B.There may be a number of reasons:\n",
    "Lower Learning Rate: The second experiment has a much smaller learning rate (0.005 vs 0.050), which might have slowed down convergence and led to underfitting within the 20 epochs available.\n",
    "Inadequate Epochs: With a reduced learning rate, the model might need more epochs to reach an optimal solution.\n",
    "Data Quality: If the additional training data are noisy or non-representative, they may damage generalization rather than improve it.\n",
    "\n",
    "C. Yes, it is possible. The enhancements could be because of\n",
    "Increasing Epochs: Giving the model enough time to converge, especially with low learning rates.\n",
    "Batch Size Adjustment: A good balance between stability and convergence speed.\n",
    "Regularization Techniques: Using dropout, batch normalization, or L2 regularization to prevent overfitting.\n",
    "More Complex Model: If the model is overly simple, adding capacity (i.e., additional layers or neurons) might help capture more complex patterns in data.\n",
    "\n",
    "D.No, training the model for so many epochs isn't always helpful. \n",
    "Risk of Overfitting: Whenever the model trains for so many epochs, the model starts remembering the training examples and not adapting to new ones.\n",
    "Computational Cost: Longer training takes more time and resources without guaranteed improvement.\n",
    "A good approach is to use early stopping, where the training is stopped when validation loss starts to increase, indicating overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW2_The_Perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
